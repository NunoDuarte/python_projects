{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2c7c21cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'random',\n",
      " 'metric': {'goal': 'minimize', 'name': 'loss'},\n",
      " 'parameters': {'batch_size': {'distribution': 'q_log_uniform_values',\n",
      "                               'max': 256,\n",
      "                               'min': 32,\n",
      "                               'q': 8},\n",
      "                'dropout': {'values': [0.3, 0.4, 0.5]},\n",
      "                'epochs': {'value': 2},\n",
      "                'fc_layer_size': {'values': [128, 256, 512]},\n",
      "                'learning_rate': {'distribution': 'uniform',\n",
      "                                  'max': 0.1,\n",
      "                                  'min': 0},\n",
      "                'optimizer': {'values': ['adam', 'sgd']}}}\n",
      "Create sweep with ID: hb9e9nup\n",
      "Sweep URL: https://wandb.ai/nunoduarte/digitrecognizer-sweeps/sweeps/hb9e9nup\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['adam', 'sgd']\n",
    "        },\n",
    "    'fc_layer_size': {\n",
    "        'values': [128, 256, 512]\n",
    "        },\n",
    "    'dropout': {\n",
    "          'values': [0.3, 0.4, 0.5]\n",
    "        },\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "parameters_dict.update({\n",
    "    'epochs': {\n",
    "        'value': 2}\n",
    "    })\n",
    "\n",
    "parameters_dict.update({\n",
    "    'learning_rate': {\n",
    "        # a flat distribution between 0 and 0.1\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0,\n",
    "        'max': 0.1\n",
    "      },\n",
    "    'batch_size': {\n",
    "        # integers between 32 and 256\n",
    "        # with evenly-distributed logarithms \n",
    "        'distribution': 'q_log_uniform_values',\n",
    "        'q': 8,\n",
    "        'min': 32,\n",
    "        'max': 256,\n",
    "      }\n",
    "    })\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(sweep_config)\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"digitrecognizer-sweeps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "76d09366-4c0c-46ec-b80a-096c1275f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# creating tensor from targets_df \n",
    "df_x = df.iloc[:,1:]\n",
    "df_y = df.iloc[:,0]\n",
    "# normalize from 0:1\n",
    "torch_x = torch.tensor(df_x.values).float() / 255\n",
    "torch_y = torch.tensor(df_y.values).long()\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "num_classes = 10  # Assuming you have 10 classes (0 to 9)\n",
    "one_hot_encoded = torch.eye(num_classes)[torch_y]\n",
    "\n",
    "# split train, val, test set\n",
    "split = 38000\n",
    "train_x = torch_x[:split]\n",
    "train_y = torch_y[:split]\n",
    "# train_y = one_hot_encoded[:split]\n",
    "\n",
    "val_x = torch_x[split:]\n",
    "val_y = torch_y[split:]\n",
    "# val_y = one_hot_encoded[split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a317148b-51e7-4aa8-95d4-e74efd4378f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(fc_layer_size, dropout):\n",
    "    model = nn.Sequential(  # fully-connected\n",
    "        nn.Linear(28*28, fc_layer_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(fc_layer_size, fc_layer_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(fc_layer_size, 10))\n",
    "\n",
    "    return model.cuda()\n",
    "        \n",
    "\n",
    "def build_optimizer(model, optimizer, learning_rate):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                              lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(model.parameters(),\n",
    "                               lr=learning_rate)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def train_epoch(network, train_loader, val_loader, optimizer, epoch):\n",
    "    losses = list()\n",
    "    accuracy = list()\n",
    "    cumu_loss_t = 0\n",
    "    for batch in train_loader():\n",
    "        x, y = batch\n",
    "        \n",
    "        # b = x.size(0)\n",
    "        # x = x.view(b, -1)   \n",
    "\n",
    "        l = model(x.cuda())    # l:logits\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        # 2. compute the objective function\n",
    "        J = loss(l, y.cuda())\n",
    "\n",
    "        # 3. cleaning the gradients\n",
    "        model.zero_grad()\n",
    "            \n",
    "        # 4. accumulate the partial derivatives of J wrt params\n",
    "        J.backward()\n",
    "\n",
    "        # 5. step in hte opposite direction of the gradient\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(J.item())\n",
    "        cumu_loss_t += J.item()\n",
    "        accuracy.append(y.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
    "        \n",
    "        wandb.log({\"batch loss\": J.item()})\n",
    "\n",
    "    print(f' Epoch {epoch +1}, train loss: {torch.tensor(losses).mean()}', end=', ')\n",
    "    print(f' train accuracy: {torch.tensor(accuracy).mean()}')\n",
    "\n",
    "    losses = list()\n",
    "    accuracy = list()\n",
    "    cumu_loss = 0\n",
    "    for batch in val_loader():\n",
    "        x, y = batch\n",
    "        \n",
    "        # # for torchvision dataset\n",
    "        # b = x.size(0)\n",
    "        # x = x.view(b, -1)\n",
    "\n",
    "        # 1. forward\n",
    "        with torch.no_grad():\n",
    "            l = model(x.cuda())    # l:logits\n",
    "\n",
    "        # 2. compute the objective function\n",
    "        J = loss(l, y.cuda())\n",
    "\n",
    "        losses.append(J.item())\n",
    "        cumu_loss += J.item()\n",
    "        accuracy.append(y.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
    "\n",
    "    print(f' Epoch {epoch +1}, validation loss: {torch.tensor(losses).mean()}', end=', ')\n",
    "    print(f' val accuracy: {torch.tensor(accuracy).mean()}')\n",
    "\n",
    "    return cumu_loss_t, cumu_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "54127ac7-0186-481d-9626-3eee3565cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        def train_loader():\n",
    "            num_batches = train_x.shape[0] // config.batch_size\n",
    "            for i in range(num_batches):\n",
    "                batch_start = i * config.batch_size\n",
    "                batch_end = (i + 1) * config.batch_size\n",
    "                batch_X = train_x[batch_start:batch_end,:]\n",
    "                batch_Y = train_y[batch_start:batch_end]\n",
    "                yield batch_X, batch_Y\n",
    "\n",
    "        def val_loader():\n",
    "            num_batches = val_x.shape[0] // config.batch_size\n",
    "            for i in range(num_batches):\n",
    "                batch_start = i * config.batch_size\n",
    "                batch_end = (i + 1) * config.batch_size\n",
    "                batch_X = val_x[batch_start:batch_end,:]\n",
    "                batch_Y = val_y[batch_start:batch_end]\n",
    "                yield batch_X, batch_Y\n",
    "\n",
    "        network = build_network(config.fc_layer_size, config.dropout)\n",
    "        optimizer = build_optimizer(network, config.optimizer, config.learning_rate)\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            avg_train_loss, avg_val_loss = train_epoch(network, train_loader, val_loader, optimizer, epoch)\n",
    "            wandb.log({\"loss\": avg_train_loss, \"val loss\": avg_val_loss, \"epoch\": epoch})           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "24044f86-7d88-4e8c-b688-cd5acc0cc64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p7udbjql with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 112\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layer_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.030106120876737297\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/home/nuno/Downloads/digit-recognizer/wandb/run-20240703_164055-p7udbjql</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/p7udbjql' target=\"_blank\">sunny-sweep-12</a></strong> to <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/sweeps/hb9e9nup' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps/sweeps/hb9e9nup</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/sweeps/hb9e9nup' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps/sweeps/hb9e9nup</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/p7udbjql' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/p7udbjql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1, train loss: 0.0640685185790062,  train accuracy: 0.9800094366073608\n",
      " Epoch 1, validation loss: 0.12194699048995972,  val accuracy: 0.9647958874702454\n",
      " Epoch 2, train loss: 0.0640685185790062,  train accuracy: 0.9800094366073608\n",
      " Epoch 2, validation loss: 0.12194699048995972,  val accuracy: 0.9647958874702454\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▃▃▃▃▄▂▂▄▅█▆▇▇█▁▇▁▄▃▂▃▃▅▄▁▅▂▂▂▂▃▃▂▆▄▂▁▅▅▃</td></tr><tr><td>epoch</td><td>▁█</td></tr><tr><td>loss</td><td>▁▁</td></tr><tr><td>val loss</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>0.0366</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>loss</td><td>21.71923</td></tr><tr><td>val loss</td><td>4.26814</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sunny-sweep-12</strong> at: <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/p7udbjql' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/p7udbjql</a><br/> View project at: <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240703_164055-p7udbjql/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ynwyz3ue with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0033090616601750057\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/home/nuno/Downloads/digit-recognizer/wandb/run-20240703_164111-ynwyz3ue</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/ynwyz3ue' target=\"_blank\">royal-sweep-13</a></strong> to <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/sweeps/hb9e9nup' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps/sweeps/hb9e9nup</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/sweeps/hb9e9nup' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps/sweeps/hb9e9nup</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/ynwyz3ue' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/ynwyz3ue</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1, train loss: 0.06408777832984924,  train accuracy: 0.9800010323524475\n",
      " Epoch 1, validation loss: 0.1222604289650917,  val accuracy: 0.9649697542190552\n",
      " Epoch 2, train loss: 0.06408777832984924,  train accuracy: 0.9800010323524475\n",
      " Epoch 2, validation loss: 0.1222604289650917,  val accuracy: 0.9649697542190552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▁▂▃▂▂▁▁▁▁▁▂▂▅▄▁▂▂▃▂▂▄▂▃▁▂▁▂▁█▄▁▂▃▁▂▁▁▁▂▂</td></tr><tr><td>epoch</td><td>▁█</td></tr><tr><td>loss</td><td>▁▁</td></tr><tr><td>val loss</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>0.0421</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>loss</td><td>38.00405</td></tr><tr><td>val loss</td><td>7.58015</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">royal-sweep-13</strong> at: <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/ynwyz3ue' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/ynwyz3ue</a><br/> View project at: <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240703_164111-ynwyz3ue/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 66vj3hrp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 80\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layer_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.007051036531566491\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/home/nuno/Downloads/digit-recognizer/wandb/run-20240703_164123-66vj3hrp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/66vj3hrp' target=\"_blank\">breezy-sweep-14</a></strong> to <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/sweeps/hb9e9nup' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps/sweeps/hb9e9nup</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/sweeps/hb9e9nup' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps/sweeps/hb9e9nup</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/66vj3hrp' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/66vj3hrp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1, train loss: 0.06406784802675247,  train accuracy: 0.9800000786781311\n",
      " Epoch 1, validation loss: 0.12133482098579407,  val accuracy: 0.9652499556541443\n",
      " Epoch 2, train loss: 0.06406784802675247,  train accuracy: 0.9800000786781311\n",
      " Epoch 2, validation loss: 0.12133482098579407,  val accuracy: 0.9652499556541443\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▄▃▃▃▁▁▂▂▂█▃▆█▅▃▃▁▂▃▂▁▄▁▂▁▁▁▄▂▄▄▃▂▄▂▅▄▂▄▁</td></tr><tr><td>epoch</td><td>▁█</td></tr><tr><td>loss</td><td>▁▁</td></tr><tr><td>val loss</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>0.0426</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>loss</td><td>30.43223</td></tr><tr><td>val loss</td><td>6.06674</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">breezy-sweep-14</strong> at: <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/66vj3hrp' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/66vj3hrp</a><br/> View project at: <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240703_164123-66vj3hrp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 61xnv9x1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layer_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.09940310825863936\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/home/nuno/Downloads/digit-recognizer/wandb/run-20240703_164133-61xnv9x1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/61xnv9x1' target=\"_blank\">vague-sweep-15</a></strong> to <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/sweeps/hb9e9nup' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps/sweeps/hb9e9nup</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/sweeps/hb9e9nup' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps/sweeps/hb9e9nup</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/61xnv9x1' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/61xnv9x1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1, train loss: 0.06408625841140747,  train accuracy: 0.9799967408180237\n",
      " Epoch 1, validation loss: 0.1229841485619545,  val accuracy: 0.9647565484046936\n",
      " Epoch 2, train loss: 0.06408625841140747,  train accuracy: 0.9799967408180237\n",
      " Epoch 2, validation loss: 0.1229841485619545,  val accuracy: 0.9647565484046936\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▂▁▃▃█▂▃▂▅▂▄▅▁▃▂▄▄▅▂▄▃▃▃▄▄▁▁▁▃▃▅▃▂▇▅▄▂▄▃▅</td></tr><tr><td>epoch</td><td>▁█</td></tr><tr><td>loss</td><td>▁▁</td></tr><tr><td>val loss</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>0.04226</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>loss</td><td>17.88007</td></tr><tr><td>val loss</td><td>3.56654</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vague-sweep-15</strong> at: <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/61xnv9x1' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/61xnv9x1</a><br/> View project at: <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240703_164133-61xnv9x1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: t4ck3fb1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layer_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.003257377582173338\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/home/nuno/Downloads/digit-recognizer/wandb/run-20240703_164143-t4ck3fb1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/t4ck3fb1' target=\"_blank\">misunderstood-sweep-16</a></strong> to <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/sweeps/hb9e9nup' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps/sweeps/hb9e9nup</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/sweeps/hb9e9nup' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps/sweeps/hb9e9nup</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/t4ck3fb1' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/t4ck3fb1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1, train loss: 0.06404935568571091,  train accuracy: 0.980017900466919\n",
      " Epoch 1, validation loss: 0.12133480608463287,  val accuracy: 0.9652500152587891\n",
      " Epoch 2, train loss: 0.06404935568571091,  train accuracy: 0.980017900466919\n",
      " Epoch 2, validation loss: 0.12133480608463287,  val accuracy: 0.9652500152587891\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>▂▂▁▅▁▁▁▁▂▁▃▂▆▅▂▁▄▆▁▂█▁▁▂▂▁▃▁▃▆▁▂▆▁▃▁▂▁▃▁</td></tr><tr><td>epoch</td><td>▁█</td></tr><tr><td>loss</td><td>▁▁</td></tr><tr><td>val loss</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>0.01848</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>loss</td><td>76.02658</td></tr><tr><td>val loss</td><td>15.16685</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">misunderstood-sweep-16</strong> at: <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/t4ck3fb1' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps/runs/t4ck3fb1</a><br/> View project at: <a href='https://wandb.ai/nunoduarte/digitrecognizer-sweeps' target=\"_blank\">https://wandb.ai/nunoduarte/digitrecognizer-sweeps</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240703_164143-t4ck3fb1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train, count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "436d6c1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1, train loss: 0.46435442566871643,  train accuracy: 0.8583350777626038\n",
      " Epoch 1, validation loss: 0.21963706612586975,  val accuracy: 0.9387500286102295\n",
      " Epoch 2, train loss: 0.18161554634571075,  train accuracy: 0.9446872472763062\n",
      " Epoch 2, validation loss: 0.16401249170303345,  val accuracy: 0.9522500038146973\n",
      " Epoch 3, train loss: 0.12903910875320435,  train accuracy: 0.9611151814460754\n",
      " Epoch 3, validation loss: 0.13805542886257172,  val accuracy: 0.9585000276565552\n",
      " Epoch 4, train loss: 0.10031295567750931,  train accuracy: 0.9703559279441833\n",
      " Epoch 4, validation loss: 0.1297679841518402,  val accuracy: 0.9610000252723694\n",
      " Epoch 5, train loss: 0.08099465072154999,  train accuracy: 0.9761478304862976\n",
      " Epoch 5, validation loss: 0.13459056615829468,  val accuracy: 0.9607499837875366\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m J\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# 5. step in hte opposite direction of the gradient\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(J\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     38\u001b[0m accuracy\u001b[38;5;241m.\u001b[39mappend(y\u001b[38;5;241m.\u001b[39meq(l\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu())\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean())\n",
      "File \u001b[0;32m~/envs/kaggle/lib/python3.8/site-packages/torch/optim/optimizer.py:392\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m post_hook \u001b[38;5;129;01min\u001b[39;00m chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_post_hooks\u001b[38;5;241m.\u001b[39mvalues(), _global_optimizer_post_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    390\u001b[0m     post_hook(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[0;32m--> 392\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/envs/kaggle/lib/python3.8/site-packages/torch/autograd/profiler.py:622\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m--> 622\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_exit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RecordFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    624\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[0;32m~/envs/kaggle/lib/python3.8/site-packages/torch/_ops.py:513\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# My Training loops\n",
    "nb_epochs = 50\n",
    "for epoch in range(nb_epochs):\n",
    "    losses = list()\n",
    "    accuracy = list()\n",
    "    for batch in train_loader():\n",
    "        x, y = batch\n",
    "\n",
    "        # print(x.size())\n",
    "        # print(y)\n",
    "\n",
    "        # # for torchvision dataset\n",
    "        b = x.size(0)\n",
    "        x = x.view(b, -1)\n",
    "\n",
    "        # print(x.size())\n",
    "\n",
    "        # batch b\n",
    "        # x 28*28\n",
    "\n",
    "        # 1. forward\n",
    "        # print(x[0])\n",
    "        l = model(x.cuda())    # l:logits\n",
    "\n",
    "        # 2. compute the objective function\n",
    "        J = loss(l, y.cuda())\n",
    "\n",
    "        # 3. cleaning the gradients\n",
    "        model.zero_grad()\n",
    "            \n",
    "        # 4. accumulate the partial derivatives of J wrt params\n",
    "        J.backward()\n",
    "\n",
    "        # 5. step in hte opposite direction of the gradient\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(J.item())\n",
    "        accuracy.append(y.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
    "\n",
    "    print(f' Epoch {epoch +1}, train loss: {torch.tensor(losses).mean()}', end=', ')\n",
    "    print(f' train accuracy: {torch.tensor(accuracy).mean()}')\n",
    "\n",
    "\n",
    "    losses = list()\n",
    "    accuracy = list()\n",
    "    for batch in val_loader():\n",
    "        x, y = batch\n",
    "        \n",
    "        # for torchvision dataset\n",
    "        b = x.size(0)\n",
    "        x = x.view(b, -1)\n",
    "\n",
    "        # 1. forward\n",
    "        with torch.no_grad():\n",
    "            l = model(x.cuda())    # l:logits\n",
    "\n",
    "        # 2. compute the objective function\n",
    "        J = loss(l, y.cuda())\n",
    "\n",
    "        losses.append(J.item())\n",
    "        accuracy.append(y.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
    "\n",
    "    print(f' Epoch {epoch +1}, validation loss: {torch.tensor(losses).mean()}', end=', ')\n",
    "    print(f' val accuracy: {torch.tensor(accuracy).mean()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1e781f7-c33f-47e3-bc63-5d05a4c5472b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42000, 784])\n",
      "torch.Size([42000])\n",
      "torch.Size([38000, 1, 28, 28])\n",
      "torch.Size([4000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# printing out result\n",
    "print(torch_x.shape)\n",
    "print(torch_y.shape)\n",
    "\n",
    "torch_x_image = torch_x.view(-1, 1, 28, 28)\n",
    "\n",
    "# split train, val, test set\n",
    "split = 38000\n",
    "train_x_image = torch_x_image[:split]\n",
    "train_y = torch_y[:split]\n",
    "# train_y = one_hot_encoded[:split]\n",
    "\n",
    "val_x_image = torch_x_image[split:]\n",
    "val_y = torch_y[split:]\n",
    "# val_y = one_hot_encoded[split:]\n",
    "\n",
    "# printing out result\n",
    "print(train_x_image.shape)\n",
    "print(val_x_image.shape)\n",
    "\n",
    "# batching data\n",
    "batch_size = 32\n",
    "def train_loader():\n",
    "    num_batches = train_x.shape[0] // batch_size\n",
    "    for i in range(num_batches):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = (i + 1) * batch_size\n",
    "        batch_X_image = train_x_image[batch_start:batch_end,:]\n",
    "        batch_Y = train_y[batch_start:batch_end]\n",
    "        yield batch_X_image, batch_Y\n",
    "\n",
    "def val_loader():\n",
    "    num_batches = val_x.shape[0] // batch_size\n",
    "    for i in range(num_batches):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = (i + 1) * batch_size\n",
    "        batch_X_image = val_x_image[batch_start:batch_end,:]\n",
    "        batch_Y = val_y[batch_start:batch_end]\n",
    "        yield batch_X_image, batch_Y\n",
    "        \n",
    "# Creating a CNN class\n",
    "class ConvNeuralNet(nn.Module):\n",
    "\t#  Determine what layers and their order in CNN object \n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNeuralNet, self).__init__()\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1024, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    # Progresses data across layers    \n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer1(x)\n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.max_pool1(out)\n",
    "        \n",
    "        out = self.conv_layer3(out)\n",
    "        out = self.conv_layer4(out)\n",
    "        out = self.max_pool2(out)\n",
    "                \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNeuralNet(10).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e935f2dc-e668-49cf-9575-c9dadfe60817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1, train loss: 0.2350294440984726,  train accuracy: 0.9244155287742615\n",
      " Epoch 1, validation loss: 0.09493966400623322,  val accuracy: 0.9670000076293945\n",
      " Epoch 2, train loss: 0.06232268735766411,  train accuracy: 0.9809130430221558\n",
      " Epoch 2, validation loss: 0.07293308526277542,  val accuracy: 0.9782500267028809\n",
      " Epoch 3, train loss: 0.041058119386434555,  train accuracy: 0.9876000285148621\n",
      " Epoch 3, validation loss: 0.07747603207826614,  val accuracy: 0.9787499904632568\n",
      " Epoch 4, train loss: 0.03091043420135975,  train accuracy: 0.9905223250389099\n",
      " Epoch 4, validation loss: 0.0742223709821701,  val accuracy: 0.981249988079071\n",
      " Epoch 5, train loss: 0.022718852385878563,  train accuracy: 0.9927864074707031\n",
      " Epoch 5, validation loss: 0.09567372500896454,  val accuracy: 0.9769999980926514\n",
      " Epoch 6, train loss: 0.017699792981147766,  train accuracy: 0.9942607283592224\n",
      " Epoch 6, validation loss: 0.0882871225476265,  val accuracy: 0.9789999723434448\n",
      " Epoch 7, train loss: 0.016401339322328568,  train accuracy: 0.9945240020751953\n",
      " Epoch 7, validation loss: 0.05485977977514267,  val accuracy: 0.9857500195503235\n",
      " Epoch 8, train loss: 0.015868131071329117,  train accuracy: 0.9947609305381775\n",
      " Epoch 8, validation loss: 0.043639570474624634,  val accuracy: 0.9879999756813049\n",
      " Epoch 9, train loss: 0.011948841623961926,  train accuracy: 0.9959983229637146\n",
      " Epoch 9, validation loss: 0.045554742217063904,  val accuracy: 0.9890000224113464\n",
      " Epoch 10, train loss: 0.012688166461884975,  train accuracy: 0.9955770969390869\n",
      " Epoch 10, validation loss: 0.05291006714105606,  val accuracy: 0.9884999990463257\n",
      " Epoch 11, train loss: 0.010036840103566647,  train accuracy: 0.9965775012969971\n",
      " Epoch 11, validation loss: 0.07279392331838608,  val accuracy: 0.9867500066757202\n",
      " Epoch 12, train loss: 0.006943236105144024,  train accuracy: 0.9979465007781982\n",
      " Epoch 12, validation loss: 0.0438624769449234,  val accuracy: 0.9912499785423279\n",
      " Epoch 13, train loss: 0.007414068095386028,  train accuracy: 0.9975252747535706\n",
      " Epoch 13, validation loss: 0.07314866781234741,  val accuracy: 0.9884999990463257\n",
      " Epoch 14, train loss: 0.0077033634297549725,  train accuracy: 0.9974726438522339\n",
      " Epoch 14, validation loss: 0.062281426042318344,  val accuracy: 0.9882500171661377\n",
      " Epoch 15, train loss: 0.004987927153706551,  train accuracy: 0.9983413815498352\n",
      " Epoch 15, validation loss: 0.04654702916741371,  val accuracy: 0.9897500276565552\n",
      " Epoch 16, train loss: 0.005193608347326517,  train accuracy: 0.9984467029571533\n",
      " Epoch 16, validation loss: 0.08033160120248795,  val accuracy: 0.9837499856948853\n",
      " Epoch 17, train loss: 0.003402004251256585,  train accuracy: 0.9987889528274536\n",
      " Epoch 17, validation loss: 0.0482182614505291,  val accuracy: 0.9897500276565552\n",
      " Epoch 18, train loss: 0.004924273118376732,  train accuracy: 0.9987099766731262\n",
      " Epoch 18, validation loss: 0.05425851792097092,  val accuracy: 0.9887499809265137\n",
      " Epoch 19, train loss: 0.004501926247030497,  train accuracy: 0.9984204173088074\n",
      " Epoch 19, validation loss: 0.06876258552074432,  val accuracy: 0.9884999990463257\n",
      " Epoch 20, train loss: 0.009264118038117886,  train accuracy: 0.9973673224449158\n",
      " Epoch 20, validation loss: 0.05820140987634659,  val accuracy: 0.987500011920929\n",
      " Epoch 21, train loss: 0.002682820428162813,  train accuracy: 0.9990785717964172\n",
      " Epoch 21, validation loss: 0.045671600848436356,  val accuracy: 0.9900000095367432\n",
      " Epoch 22, train loss: 0.0017564730951562524,  train accuracy: 0.9992628693580627\n",
      " Epoch 22, validation loss: 0.062381450086832047,  val accuracy: 0.9900000095367432\n",
      " Epoch 23, train loss: 0.005082589108496904,  train accuracy: 0.9984204173088074\n",
      " Epoch 23, validation loss: 0.08277376741170883,  val accuracy: 0.9879999756813049\n",
      " Epoch 24, train loss: 0.007983517833054066,  train accuracy: 0.9975252747535706\n",
      " Epoch 24, validation loss: 0.05818208307027817,  val accuracy: 0.9869999885559082\n",
      " Epoch 25, train loss: 0.00790029764175415,  train accuracy: 0.9975779056549072\n",
      " Epoch 25, validation loss: 0.06139224022626877,  val accuracy: 0.9892500042915344\n",
      " Epoch 26, train loss: 0.005186437629163265,  train accuracy: 0.9982361197471619\n",
      " Epoch 26, validation loss: 0.05287494882941246,  val accuracy: 0.9897500276565552\n",
      " Epoch 27, train loss: 0.002903613494709134,  train accuracy: 0.9991048574447632\n",
      " Epoch 27, validation loss: 0.05465211346745491,  val accuracy: 0.9882500171661377\n",
      " Epoch 28, train loss: 0.0026703784242272377,  train accuracy: 0.9991575479507446\n",
      " Epoch 28, validation loss: 0.06961288303136826,  val accuracy: 0.987500011920929\n",
      " Epoch 29, train loss: 0.006977344863116741,  train accuracy: 0.9980254769325256\n",
      " Epoch 29, validation loss: 0.05112823471426964,  val accuracy: 0.9915000200271606\n",
      " Epoch 30, train loss: 0.005107149947434664,  train accuracy: 0.9981834292411804\n",
      " Epoch 30, validation loss: 0.06442204862833023,  val accuracy: 0.9884999990463257\n",
      " Epoch 31, train loss: 0.004679540637880564,  train accuracy: 0.9986836314201355\n",
      " Epoch 31, validation loss: 0.057309530675411224,  val accuracy: 0.9904999732971191\n",
      " Epoch 32, train loss: 0.0027951570227742195,  train accuracy: 0.999236524105072\n",
      " Epoch 32, validation loss: 0.08622067421674728,  val accuracy: 0.984000027179718\n",
      " Epoch 33, train loss: 0.0013466031523421407,  train accuracy: 0.9996840953826904\n",
      " Epoch 33, validation loss: 0.04746248573064804,  val accuracy: 0.9922500252723694\n",
      " Epoch 34, train loss: 8.908670133678243e-05,  train accuracy: 1.0\n",
      " Epoch 34, validation loss: 0.04664219170808792,  val accuracy: 0.9919999837875366\n",
      " Epoch 35, train loss: 3.646973709692247e-05,  train accuracy: 1.0\n",
      " Epoch 35, validation loss: 0.04739546403288841,  val accuracy: 0.9929999709129333\n",
      " Epoch 36, train loss: 1.2410024282871746e-05,  train accuracy: 1.0\n",
      " Epoch 36, validation loss: 0.04742008447647095,  val accuracy: 0.9932500123977661\n",
      " Epoch 37, train loss: 9.626450264477171e-06,  train accuracy: 1.0\n",
      " Epoch 37, validation loss: 0.0475650429725647,  val accuracy: 0.9932500123977661\n",
      " Epoch 38, train loss: 8.331551725859754e-06,  train accuracy: 1.0\n",
      " Epoch 38, validation loss: 0.04768972098827362,  val accuracy: 0.9932500123977661\n",
      " Epoch 39, train loss: 7.345205631281715e-06,  train accuracy: 1.0\n",
      " Epoch 39, validation loss: 0.047822706401348114,  val accuracy: 0.9932500123977661\n",
      " Epoch 40, train loss: 6.5529870880709495e-06,  train accuracy: 1.0\n",
      " Epoch 40, validation loss: 0.04798434302210808,  val accuracy: 0.9932500123977661\n",
      " Epoch 41, train loss: 6.020746241119923e-06,  train accuracy: 1.0\n",
      " Epoch 41, validation loss: 0.04811131954193115,  val accuracy: 0.9932500123977661\n",
      " Epoch 42, train loss: 5.524017979041673e-06,  train accuracy: 1.0\n",
      " Epoch 42, validation loss: 0.04824947565793991,  val accuracy: 0.9932500123977661\n",
      " Epoch 43, train loss: 5.101677288621431e-06,  train accuracy: 1.0\n",
      " Epoch 43, validation loss: 0.048394542187452316,  val accuracy: 0.9934999942779541\n",
      " Epoch 44, train loss: 4.789324520970695e-06,  train accuracy: 1.0\n",
      " Epoch 44, validation loss: 0.04851948469877243,  val accuracy: 0.9932500123977661\n",
      " Epoch 45, train loss: 4.482186341192573e-06,  train accuracy: 1.0\n",
      " Epoch 45, validation loss: 0.048644863069057465,  val accuracy: 0.9932500123977661\n",
      " Epoch 46, train loss: 4.225167231197702e-06,  train accuracy: 1.0\n",
      " Epoch 46, validation loss: 0.04875800758600235,  val accuracy: 0.9932500123977661\n",
      " Epoch 47, train loss: 3.981838290201267e-06,  train accuracy: 1.0\n",
      " Epoch 47, validation loss: 0.04889339581131935,  val accuracy: 0.9932500123977661\n",
      " Epoch 48, train loss: 3.7975678424118087e-06,  train accuracy: 1.0\n",
      " Epoch 48, validation loss: 0.048992834985256195,  val accuracy: 0.9932500123977661\n",
      " Epoch 49, train loss: 3.605246547522256e-06,  train accuracy: 1.0\n",
      " Epoch 49, validation loss: 0.04910292476415634,  val accuracy: 0.9932500123977661\n",
      " Epoch 50, train loss: 3.437618715906865e-06,  train accuracy: 1.0\n",
      " Epoch 50, validation loss: 0.049212392419576645,  val accuracy: 0.9932500123977661\n"
     ]
    }
   ],
   "source": [
    "# My Training loops\n",
    "nb_epochs = 50\n",
    "for epoch in range(nb_epochs):\n",
    "    losses = list()\n",
    "    accuracy = list()\n",
    "    for batch in train_loader():\n",
    "        x, y = batch\n",
    "\n",
    "        # print(x.size())\n",
    "        # print(y)\n",
    "\n",
    "        # # for torchvision dataset\n",
    "        # b = x.size(0)\n",
    "        # x = x.view(b, -1)\n",
    "\n",
    "        # print(x.size())\n",
    "\n",
    "        # batch b\n",
    "        # x 28*28\n",
    "\n",
    "        # 1. forward\n",
    "        # print(x[0])\n",
    "        l = model(x.cuda())    # l:logits\n",
    "\n",
    "        # 2. compute the objective function\n",
    "        J = loss(l, y.cuda())\n",
    "\n",
    "        # 3. cleaning the gradients\n",
    "        model.zero_grad()\n",
    "            \n",
    "        # 4. accumulate the partial derivatives of J wrt params\n",
    "        J.backward()\n",
    "\n",
    "        # 5. step in hte opposite direction of the gradient\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(J.item())\n",
    "        accuracy.append(y.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
    "\n",
    "    print(f' Epoch {epoch +1}, train loss: {torch.tensor(losses).mean()}', end=', ')\n",
    "    print(f' train accuracy: {torch.tensor(accuracy).mean()}')\n",
    "\n",
    "\n",
    "    losses = list()\n",
    "    accuracy = list()\n",
    "    for batch in val_loader():\n",
    "        x, y = batch\n",
    "        \n",
    "        # # for torchvision dataset\n",
    "        # b = x.size(0)\n",
    "        # x = x.view(b, -1)\n",
    "\n",
    "        # 1. forward\n",
    "        with torch.no_grad():\n",
    "            l = model(x.cuda())    # l:logits\n",
    "\n",
    "        # 2. compute the objective function\n",
    "        J = loss(l, y.cuda())\n",
    "\n",
    "        losses.append(J.item())\n",
    "        accuracy.append(y.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
    "\n",
    "    print(f' Epoch {epoch +1}, validation loss: {torch.tensor(losses).mean()}', end=', ')\n",
    "    print(f' val accuracy: {torch.tensor(accuracy).mean()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b20e5caa-b395-40af-8727-1e6942a104ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more flexible model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(28 * 28, 64)\n",
    "        self.l2 = nn.Linear(64, 64)\n",
    "        self.l3 = nn.Linear(64, 10)\n",
    "        self.do = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = nn.functional.relu(self.l1(x))\n",
    "        h2 = nn.functional.relu(self.l2(h1))\n",
    "        do = self.do(h1 + h2)  # this allows for the partial gradients in the deeper layers (first ones) to update faster\n",
    "        logits = self.l3(do)\n",
    "        return logits\n",
    "\n",
    "model = ResNet().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cd1cdc-f854-4cdd-9e66-0802bdb2b15a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bde62f-ddc0-45c5-b30b-02ad4453f6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
