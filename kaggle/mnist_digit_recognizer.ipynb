{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c7c21cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8c1989c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "41995      0       0       0       0       0       0       0       0       0   \n",
      "41996      1       0       0       0       0       0       0       0       0   \n",
      "41997      7       0       0       0       0       0       0       0       0   \n",
      "41998      6       0       0       0       0       0       0       0       0   \n",
      "41999      9       0       0       0       0       0       0       0       0   \n",
      "\n",
      "       pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
      "41995       0  ...         0         0         0         0         0   \n",
      "41996       0  ...         0         0         0         0         0   \n",
      "41997       0  ...         0         0         0         0         0   \n",
      "41998       0  ...         0         0         0         0         0   \n",
      "41999       0  ...         0         0         0         0         0   \n",
      "\n",
      "       pixel779  pixel780  pixel781  pixel782  pixel783  \n",
      "41995         0         0         0         0         0  \n",
      "41996         0         0         0         0         0  \n",
      "41997         0         0         0         0         0  \n",
      "41998         0         0         0         0         0  \n",
      "41999         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n",
      "torch.Size([42000, 10])\n",
      "torch.Size([42000, 784])\n",
      "torch.Size([42000])\n",
      "torch.Size([38000, 784])\n",
      "torch.Size([4000, 784])\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "print(df.tail()) \n",
    "\n",
    "# creating tensor from targets_df \n",
    "df_x = df.iloc[:,1:]\n",
    "df_y = df.iloc[:,0]\n",
    "# normalize from 0:1\n",
    "torch_x = torch.tensor(df_x.values).float() / 255\n",
    "torch_y = torch.tensor(df_y.values).long()\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "num_classes = 10  # Assuming you have 10 classes (0 to 9)\n",
    "one_hot_encoded = torch.eye(num_classes)[torch_y]\n",
    "\n",
    "print(one_hot_encoded.shape)\n",
    "\n",
    "# printing out result\n",
    "print(torch_x.shape)\n",
    "print(torch_y.shape)\n",
    "\n",
    "# split train, val, test set\n",
    "split = 38000\n",
    "train_x = torch_x[:split]\n",
    "train_y = torch_y[:split]\n",
    "# train_y = one_hot_encoded[:split]\n",
    "\n",
    "val_x = torch_x[split:]\n",
    "val_y = torch_y[split:]\n",
    "# val_y = one_hot_encoded[split:]\n",
    "\n",
    "# printing out result\n",
    "print(train_x.shape)\n",
    "print(val_x.shape)\n",
    "\n",
    "# batching data\n",
    "batch_size = 32\n",
    "def train_loader():\n",
    "    num_batches = train_x.shape[0] // batch_size\n",
    "    for i in range(num_batches):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = (i + 1) * batch_size\n",
    "        batch_X = train_x[batch_start:batch_end,:]\n",
    "        batch_Y = train_y[batch_start:batch_end]\n",
    "        yield batch_X, batch_Y\n",
    "\n",
    "def val_loader():\n",
    "    num_batches = val_x.shape[0] // batch_size\n",
    "    for i in range(num_batches):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = (i + 1) * batch_size\n",
    "        batch_X = val_x[batch_start:batch_end,:]\n",
    "        batch_Y = val_y[batch_start:batch_end]\n",
    "        yield batch_X, batch_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f68de4f9-efc6-400c-94a8-b464f3726ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process test set\n",
    "# load the data\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# creating tensor from targets_df \n",
    "df_test_x = df_test.iloc[:,:]\n",
    "\n",
    "# normalize from 0:1\n",
    "test_x = torch.tensor(df_test_x.values).float() / 255\n",
    "\n",
    "# I dont have to do this; you are just testing, no need to batch\n",
    "def test_loader():\n",
    "    num_batches = test_x.shape[0] // batch_size\n",
    "    for i in range(num_batches):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = (i + 1) * batch_size\n",
    "        batch_X = test_x[batch_start:batch_end,:]\n",
    "        yield batch_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "586fa82d-a26e-4a01-a438-9868613de47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import datasets, transforms\n",
    "# from torch.utils.data import random_split, DataLoader\n",
    "# train_data = datasets.MNIST('data', train=True, download=False, transform=transforms.ToTensor())\n",
    "# train, val = random_split(train_data, [55000, 5000])\n",
    "\n",
    "# # Access a sample data point\n",
    "# sample_data = train[2]\n",
    "\n",
    "# print(sample_data)\n",
    "# # Get the shape of the sample data\n",
    "# sample_data_shape = sample_data[0].shape\n",
    "# print(\"Shape of the sample data:\", sample_data_shape)\n",
    "\n",
    "# train = train[:42000]\n",
    "# val = val[:4000]\n",
    "# train_loader = DataLoader(train, batch_size=32)\n",
    "# val_loader = DataLoader(val, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cac64631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(28*28, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(0.1),    \n",
    "    nn.Linear(64, 10)\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6f2edd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define my optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "81b1e1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "436d6c1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1, train loss: 0.390991747379303,  train accuracy: 0.8842670321464539\n",
      " Epoch 1, validation loss: 0.2302495688199997,  val accuracy: 0.9300000071525574\n",
      " Epoch 2, train loss: 0.17788442969322205,  train accuracy: 0.9457666277885437\n",
      " Epoch 2, validation loss: 0.17465655505657196,  val accuracy: 0.9449999928474426\n",
      " Epoch 3, train loss: 0.13184835016727448,  train accuracy: 0.9592986702919006\n",
      " Epoch 3, validation loss: 0.14148761332035065,  val accuracy: 0.9567499756813049\n",
      " Epoch 4, train loss: 0.10402491688728333,  train accuracy: 0.9681708216667175\n",
      " Epoch 4, validation loss: 0.145673006772995,  val accuracy: 0.9597499966621399\n",
      " Epoch 5, train loss: 0.0867791473865509,  train accuracy: 0.9739890694618225\n",
      " Epoch 5, validation loss: 0.13472117483615875,  val accuracy: 0.9605000019073486\n",
      " Epoch 6, train loss: 0.07387236505746841,  train accuracy: 0.9763584733009338\n",
      " Epoch 6, validation loss: 0.12762469053268433,  val accuracy: 0.9617499709129333\n",
      " Epoch 7, train loss: 0.06159485876560211,  train accuracy: 0.9810446500778198\n",
      " Epoch 7, validation loss: 0.12940286099910736,  val accuracy: 0.9632499814033508\n",
      " Epoch 8, train loss: 0.054197508841753006,  train accuracy: 0.9826769232749939\n",
      " Epoch 8, validation loss: 0.1217741072177887,  val accuracy: 0.9635000228881836\n",
      " Epoch 9, train loss: 0.046980321407318115,  train accuracy: 0.9852306246757507\n",
      " Epoch 9, validation loss: 0.1481350213289261,  val accuracy: 0.9612500071525574\n",
      " Epoch 10, train loss: 0.04274272918701172,  train accuracy: 0.9860204458236694\n",
      " Epoch 10, validation loss: 0.13953164219856262,  val accuracy: 0.9637500047683716\n",
      " Epoch 11, train loss: 0.03840618580579758,  train accuracy: 0.987494707107544\n",
      " Epoch 11, validation loss: 0.1570362150669098,  val accuracy: 0.9622499942779541\n",
      " Epoch 12, train loss: 0.03418930619955063,  train accuracy: 0.9891006946563721\n",
      " Epoch 12, validation loss: 0.1534888744354248,  val accuracy: 0.9647499918937683\n",
      " Epoch 13, train loss: 0.03059176355600357,  train accuracy: 0.9901800751686096\n",
      " Epoch 13, validation loss: 0.14008311927318573,  val accuracy: 0.9622499942779541\n",
      " Epoch 14, train loss: 0.028887825086712837,  train accuracy: 0.9909435510635376\n",
      " Epoch 14, validation loss: 0.1325201541185379,  val accuracy: 0.9652500152587891\n",
      " Epoch 15, train loss: 0.026078535243868828,  train accuracy: 0.9917333722114563\n",
      " Epoch 15, validation loss: 0.13958360254764557,  val accuracy: 0.9627500176429749\n",
      " Epoch 16, train loss: 0.02364777773618698,  train accuracy: 0.9919176697731018\n",
      " Epoch 16, validation loss: 0.14650394022464752,  val accuracy: 0.965499997138977\n",
      " Epoch 17, train loss: 0.021234048530459404,  train accuracy: 0.9933919310569763\n",
      " Epoch 17, validation loss: 0.15852278470993042,  val accuracy: 0.9667500257492065\n",
      " Epoch 18, train loss: 0.022809578105807304,  train accuracy: 0.9923651814460754\n",
      " Epoch 18, validation loss: 0.13466046750545502,  val accuracy: 0.9667500257492065\n",
      " Epoch 19, train loss: 0.019424399361014366,  train accuracy: 0.993813157081604\n",
      " Epoch 19, validation loss: 0.1326082944869995,  val accuracy: 0.9679999947547913\n",
      " Epoch 20, train loss: 0.015946095809340477,  train accuracy: 0.994945228099823\n",
      " Epoch 20, validation loss: 0.16374708712100983,  val accuracy: 0.9635000228881836\n",
      " Epoch 21, train loss: 0.016004443168640137,  train accuracy: 0.9945240020751953\n",
      " Epoch 21, validation loss: 0.1481400430202484,  val accuracy: 0.9660000205039978\n",
      " Epoch 22, train loss: 0.013415640220046043,  train accuracy: 0.9955770969390869\n",
      " Epoch 22, validation loss: 0.1559407263994217,  val accuracy: 0.9667500257492065\n",
      " Epoch 23, train loss: 0.014124543406069279,  train accuracy: 0.9950242042541504\n",
      " Epoch 23, validation loss: 0.1573338657617569,  val accuracy: 0.9652500152587891\n",
      " Epoch 24, train loss: 0.012846293859183788,  train accuracy: 0.9961036443710327\n",
      " Epoch 24, validation loss: 0.1368180513381958,  val accuracy: 0.9700000286102295\n",
      " Epoch 25, train loss: 0.012507177889347076,  train accuracy: 0.9958140254020691\n",
      " Epoch 25, validation loss: 0.15773896872997284,  val accuracy: 0.965499997138977\n",
      " Epoch 26, train loss: 0.011890699155628681,  train accuracy: 0.996445894241333\n",
      " Epoch 26, validation loss: 0.1471288800239563,  val accuracy: 0.9667500257492065\n",
      " Epoch 27, train loss: 0.012023068964481354,  train accuracy: 0.9956033825874329\n",
      " Epoch 27, validation loss: 0.1585972011089325,  val accuracy: 0.9660000205039978\n",
      " Epoch 28, train loss: 0.011739642359316349,  train accuracy: 0.9963405728340149\n",
      " Epoch 28, validation loss: 0.14303117990493774,  val accuracy: 0.9660000205039978\n",
      " Epoch 29, train loss: 0.012106676585972309,  train accuracy: 0.9952874779701233\n",
      " Epoch 29, validation loss: 0.1508467048406601,  val accuracy: 0.968500018119812\n",
      " Epoch 30, train loss: 0.008914945647120476,  train accuracy: 0.9972620010375977\n",
      " Epoch 30, validation loss: 0.15084557235240936,  val accuracy: 0.9704999923706055\n",
      " Epoch 31, train loss: 0.009869497269392014,  train accuracy: 0.9968934059143066\n",
      " Epoch 31, validation loss: 0.13456806540489197,  val accuracy: 0.9712499976158142\n",
      " Epoch 32, train loss: 0.010609094053506851,  train accuracy: 0.9965775012969971\n",
      " Epoch 32, validation loss: 0.16128051280975342,  val accuracy: 0.965749979019165\n",
      " Epoch 33, train loss: 0.00980671588331461,  train accuracy: 0.9966301321983337\n",
      " Epoch 33, validation loss: 0.1597423255443573,  val accuracy: 0.9677500128746033\n",
      " Epoch 34, train loss: 0.008531440049409866,  train accuracy: 0.9973673224449158\n",
      " Epoch 34, validation loss: 0.1686200648546219,  val accuracy: 0.9662500023841858\n",
      " Epoch 35, train loss: 0.008321688510477543,  train accuracy: 0.997209370136261\n",
      " Epoch 35, validation loss: 0.1662343591451645,  val accuracy: 0.9664999842643738\n",
      " Epoch 36, train loss: 0.007995456457138062,  train accuracy: 0.9976568818092346\n",
      " Epoch 36, validation loss: 0.16787311434745789,  val accuracy: 0.9664999842643738\n",
      " Epoch 37, train loss: 0.007235776167362928,  train accuracy: 0.9973673224449158\n",
      " Epoch 37, validation loss: 0.1528286635875702,  val accuracy: 0.9695000052452087\n",
      " Epoch 38, train loss: 0.00665835477411747,  train accuracy: 0.9979465007781982\n",
      " Epoch 38, validation loss: 0.16038766503334045,  val accuracy: 0.9692500233650208\n",
      " Epoch 39, train loss: 0.007556648459285498,  train accuracy: 0.9974462985992432\n",
      " Epoch 39, validation loss: 0.15537837147712708,  val accuracy: 0.9704999923706055\n",
      " Epoch 40, train loss: 0.00951081421226263,  train accuracy: 0.9968144297599792\n",
      " Epoch 40, validation loss: 0.15278472006320953,  val accuracy: 0.972000002861023\n",
      " Epoch 41, train loss: 0.006512912921607494,  train accuracy: 0.9978148937225342\n",
      " Epoch 41, validation loss: 0.1655159294605255,  val accuracy: 0.9700000286102295\n",
      " Epoch 42, train loss: 0.007541297934949398,  train accuracy: 0.9975779056549072\n",
      " Epoch 42, validation loss: 0.17124301195144653,  val accuracy: 0.9677500128746033\n",
      " Epoch 43, train loss: 0.006157205440104008,  train accuracy: 0.9980254769325256\n",
      " Epoch 43, validation loss: 0.1732768714427948,  val accuracy: 0.96875\n",
      " Epoch 44, train loss: 0.004678027238696814,  train accuracy: 0.9986836314201355\n",
      " Epoch 44, validation loss: 0.1669822633266449,  val accuracy: 0.9702500104904175\n",
      " Epoch 45, train loss: 0.006075273733586073,  train accuracy: 0.9981307983398438\n",
      " Epoch 45, validation loss: 0.16962787508964539,  val accuracy: 0.9674999713897705\n",
      " Epoch 46, train loss: 0.005766590125858784,  train accuracy: 0.9982097744941711\n",
      " Epoch 46, validation loss: 0.18677152693271637,  val accuracy: 0.9710000157356262\n",
      " Epoch 47, train loss: 0.004165173973888159,  train accuracy: 0.9985520243644714\n",
      " Epoch 47, validation loss: 0.1757872998714447,  val accuracy: 0.9692500233650208\n",
      " Epoch 48, train loss: 0.00501115620136261,  train accuracy: 0.9983940720558167\n",
      " Epoch 48, validation loss: 0.17101530730724335,  val accuracy: 0.96875\n",
      " Epoch 49, train loss: 0.005509075243026018,  train accuracy: 0.9981834292411804\n",
      " Epoch 49, validation loss: 0.17173254489898682,  val accuracy: 0.96875\n",
      " Epoch 50, train loss: 0.00488472543656826,  train accuracy: 0.9986046552658081\n",
      " Epoch 50, validation loss: 0.1837066262960434,  val accuracy: 0.9697499871253967\n"
     ]
    }
   ],
   "source": [
    "# My Training loops\n",
    "nb_epochs = 50\n",
    "for epoch in range(nb_epochs):\n",
    "    losses = list()\n",
    "    accuracy = list()\n",
    "    for batch in train_loader():\n",
    "        x, y = batch\n",
    "\n",
    "        # print(x.size())\n",
    "        # print(y)\n",
    "\n",
    "        # # for torchvision dataset\n",
    "        b = x.size(0)\n",
    "        x = x.view(b, -1)\n",
    "\n",
    "        # print(x.size())\n",
    "\n",
    "        # batch b\n",
    "        # x 28*28\n",
    "\n",
    "        # 1. forward\n",
    "        # print(x[0])\n",
    "        l = model(x.cuda())    # l:logits\n",
    "\n",
    "        # 2. compute the objective function\n",
    "        J = loss(l, y.cuda())\n",
    "\n",
    "        # 3. cleaning the gradients\n",
    "        model.zero_grad()\n",
    "            \n",
    "        # 4. accumulate the partial derivatives of J wrt params\n",
    "        J.backward()\n",
    "\n",
    "        # 5. step in hte opposite direction of the gradient\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(J.item())\n",
    "        accuracy.append(y.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
    "\n",
    "    print(f' Epoch {epoch +1}, train loss: {torch.tensor(losses).mean()}', end=', ')\n",
    "    print(f' train accuracy: {torch.tensor(accuracy).mean()}')\n",
    "\n",
    "\n",
    "    losses = list()\n",
    "    accuracy = list()\n",
    "    for batch in val_loader():\n",
    "        x, y = batch\n",
    "        \n",
    "        # for torchvision dataset\n",
    "        b = x.size(0)\n",
    "        x = x.view(b, -1)\n",
    "\n",
    "        # 1. forward\n",
    "        with torch.no_grad():\n",
    "            l = model(x.cuda())    # l:logits\n",
    "\n",
    "        # 2. compute the objective function\n",
    "        J = loss(l, y.cuda())\n",
    "\n",
    "        losses.append(J.item())\n",
    "        accuracy.append(y.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
    "\n",
    "    print(f' Epoch {epoch +1}, validation loss: {torch.tensor(losses).mean()}', end=', ')\n",
    "    print(f' val accuracy: {torch.tensor(accuracy).mean()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "599a2df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ImageId  Label\n",
      "0        1      2\n",
      "1        2      0\n",
      "2        3      9\n",
      "3        4      9\n",
      "4        5      3\n"
     ]
    }
   ],
   "source": [
    "# evaluate on the test set\n",
    "import csv\n",
    "file_path = 'submission.csv'\n",
    "# Open the CSV file in write mode\n",
    "j = 0\n",
    "with open(file_path, 'w', newline='') as txtfile:\n",
    "    # Create a CSV writer object\n",
    "    csv_writer = csv.writer(txtfile)\n",
    "    csv_writer.writerow(['ImageId', 'Label'])\n",
    "    for batch in test_loader():\n",
    "        x = batch\n",
    "        \n",
    "        # for torchvision dataset\n",
    "        b = x.size(0)\n",
    "        x = x.view(b, -1)\n",
    "    \n",
    "        # 1. forward\n",
    "        with torch.no_grad():\n",
    "            l = model(x.cuda())    # l:logits\n",
    "    \n",
    "        out = l.detach().argmax(dim=1).cpu().float()\n",
    "        # print('output', out)\n",
    "\n",
    "        \n",
    "        for i in range(0, len(x)):\n",
    "            csv_writer.writerow([int(j+1), int(np.asarray(out[i]))])\n",
    "            j = j+1\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'submission.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b20e5caa-b395-40af-8727-1e6942a104ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more flexible model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(28 * 28, 64)\n",
    "        self.l2 = nn.Linear(64, 64)\n",
    "        self.l3 = nn.Linear(64, 10)\n",
    "        self.do = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = nn.functional.relu(self.l1(x))\n",
    "        h2 = nn.functional.relu(self.l2(h1))\n",
    "        do = self.do(h1 + h2)  # this allows for the partial gradients in the deeper layers (first ones) to update faster\n",
    "        logits = self.l3(do)\n",
    "        return logits\n",
    "\n",
    "model = ResNet().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e781f7-c33f-47e3-bc63-5d05a4c5472b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
